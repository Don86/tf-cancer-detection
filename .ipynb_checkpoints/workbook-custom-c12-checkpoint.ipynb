{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# TF learn imports\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "\n",
    "# Define some constants\n",
    "TRAIN_DIR = \"./data/train\"\n",
    "TEST_DIR = \"./data//test\"\n",
    "IMG_WIDTH = 50 # all images will be made square. Downsampled from 96\n",
    "LR = 1e-3 #learning rate\n",
    "\n",
    "ORIGINAL_SIZE = 96\n",
    "CROP_SIZE = 48 # final size after cropping\n",
    "\n",
    "dm = pd.read_csv(\"./data/train_labels.csv\")\n",
    "\n",
    "# ========= Some helper functions =========\n",
    "def normalize_img(img):\n",
    "    \"\"\"Normalizes an image to all values between 0 and 1\n",
    "    \n",
    "    PARAMS\n",
    "    ------\n",
    "    img: array of shape: (width, height, 3)\n",
    "    \n",
    "    RETURNS\n",
    "    -------\n",
    "    img_n: array of shape: (width, height, 3), normalized between 0 and 1\n",
    "    \"\"\"\n",
    "    img_n = np.zeros([CROP_SIZE, CROP_SIZE, 3])\n",
    "    start_crop = (ORIGINAL_SIZE - CROP_SIZE) // 2\n",
    "    end_crop = start_crop + CROP_SIZE\n",
    "    for c_channel in range(3):\n",
    "        channel_array = img[start_crop:end_crop, start_crop:end_crop, c_channel]/255\n",
    "        img_n[:, :, c_channel] = channel_array\n",
    "    \n",
    "    return img_n\n",
    "\n",
    "\n",
    "def create_train_data(d_meta, class_size=25000, save_fn=\"\"):\n",
    "    \"\"\"\n",
    "    Extracts sample_size data points from condition0 and condition1 *each*\n",
    "    s.t. total dataset size = 2*sample_size\n",
    "    Adds k-fold indices (k=5). 5-folds is hardcoded. \n",
    "    \"\"\"\n",
    "    # Randomly select names from the metadata set\n",
    "    d0 = d_meta.loc[d_meta[\"label\"]==0].sample(n=class_size)\n",
    "    d1 = d_meta.loc[d_meta[\"label\"]==1].sample(n=class_size)\n",
    "    selected_names_ls = [x+\".tif\" for x in list(d0[\"id\"])+list(d1[\"id\"])]\n",
    "\n",
    "    # Grab all names\n",
    "    fn_ls = os.listdir(TRAIN_DIR)\n",
    "\n",
    "    training_data = []\n",
    "    for fn in tqdm(os.listdir(TRAIN_DIR)):\n",
    "        if fn in selected_names_ls:\n",
    "            full_fn = TRAIN_DIR+\"/\"+fn\n",
    "            img = normalize_img(cv2.imread(full_fn))\n",
    "            label = dm.loc[dm[\"id\"]==fn.split(\".\")[0]].label.values[0]\n",
    "            training_data.append([fn, np.array(img), label])\n",
    "    \n",
    "    n_sets = int((class_size*2)/5)\n",
    "    k_indices_ls = [0, 1, 2, 3, 4] * n_sets\n",
    "    df = pd.DataFrame(data=training_data, columns=[\"filename\", \"data\", \"label\"])\n",
    "    df = df.sort_values(by=\"label\")\n",
    "    df[\"k-index\"] = k_indices_ls\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Save Dataset\n",
    "\n",
    "Randomly selects a subset of the original ~220k images, and preps them (normalization, cropping, splitting into folds) for input into the CNN. Saves the result, so this need only be run once. \n",
    "\n",
    "* For n=50k, the resulting df will need to be saved into multiple `pickle` files, because a single `pickle` file would be too big.\n",
    "* n=50k is actually overkill, because performance is only a little lower using n=20k. Nevertheless, let's pretend that we need n=50k, just so that this becomes an interesting engineering problem of dealing with data batching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "# Random sample of n condition0 + n condition1\n",
    "df = create_train_data(dm, class_size=25000)\n",
    "\n",
    "# check\n",
    "df.groupby([\"label\", \"k-index\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "# save each k-index separately\n",
    "# Because the whole file is too large to save as a single file\n",
    "for k in range(5):\n",
    "    d_t = df.loc[df[\"k-index\"]==k]\n",
    "    save_fn = \"data-50k-48px-fold\"+str(k)+\".pkl\"\n",
    "    d_t.to_pickle(save_fn)\n",
    "    \n",
    "print(\"Done in %.2fs\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Load the dataset created previously. If saved as multiple `.pkl` files, merge them into 1 dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and merge the 5 separate data sets\n",
    "df_ls = []\n",
    "for k in range(5):\n",
    "    fn = \"data-50k-48px-fold\"+str(k)+\".pkl\"\n",
    "    #df_dict[k] = pd.read_pickle(fn)\n",
    "    df_ls.append(pd.read_pickle(fn))\n",
    "df = pd.concat(df_ls, axis=0)\n",
    "\n",
    "# Insert sanity check printout here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep data for input into CNN\n",
    "d_train = df.loc[df[\"k-index\"].isin([1, 2, 3, 4])]\n",
    "d_test = df.loc[df[\"k-index\"]==0]\n",
    "\n",
    "X = list(d_train[\"data\"])\n",
    "X = np.array(X)\n",
    "Y = []\n",
    "for lab in list(d_train['label']):\n",
    "    if lab == 0:\n",
    "        Y.append([1, 0])\n",
    "    elif lab == 1:\n",
    "        Y.append([0, 1])\n",
    "\n",
    "x_test = list(d_test[\"data\"])\n",
    "x_test = np.array(x_test)\n",
    "y_test = []\n",
    "for lab in list(d_test['label']):\n",
    "    if lab == 0:\n",
    "        y_test.append([1, 0])\n",
    "    elif lab == 1:\n",
    "        y_test.append([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 31249  | total loss: \u001b[1m\u001b[32m0.18484\u001b[0m\u001b[0m | time: 101.815s\n",
      "| Adam | epoch: 050 | loss: 0.18484 - acc: 0.9180 -- iter: 39936/40000\n",
      "Training Step: 31250  | total loss: \u001b[1m\u001b[32m0.17571\u001b[0m\u001b[0m | time: 107.471s\n",
      "| Adam | epoch: 050 | loss: 0.17571 - acc: 0.9231 | val_loss: 0.76522 - val_acc: 0.7666 -- iter: 40000/40000\n",
      "--\n",
      "Done in 5878.27s\n"
     ]
    }
   ],
   "source": [
    "# Define model and fit\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "convnet = input_data(shape=[None, 48, 48, 3], name='input')\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = fully_connected(convnet, 512, activation='relu')\n",
    "convnet = batch_normalization(convnet)\n",
    "\n",
    "convnet = fully_connected(convnet, 2, activation='softmax')\n",
    "convnet = regression(convnet, \n",
    "                     optimizer='adam', \n",
    "                     learning_rate=LR, \n",
    "                     loss='categorical_crossentropy', \n",
    "                     name='targets')\n",
    "\n",
    "# the 2nd param logs tensorboard to /tmp\n",
    "model = tflearn.DNN(convnet, tensorboard_dir=\"summaries\")\n",
    "\n",
    "t0 = time.time()\n",
    "# Note: run_id is for tensorboard later\n",
    "model.fit({'input': X}, {'targets': Y}, \n",
    "          n_epoch=50, \n",
    "          validation_set=({'input': x_test}, {'targets': y_test}), \n",
    "          snapshot_step=500, \n",
    "          show_metric=True, \n",
    "          run_id=\"cnn-L12-run4\")\n",
    "\n",
    "print(\"Done in %.2fs\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.96666666666667"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
